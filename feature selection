# -*- coding: utf-8 -*-
"""
Created on Sat Jan 16 15:33:15 2021

@author: 阿奂
"""
import pandas as pd
import numpy as np 
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
from sklearn.datasets import load_iris

file = open('D:\Master\数据挖掘\speed_dating_train.csv')
df = pd.read_csv(file)

# 缺失值考察
def missing(data,threshold):
    percent_missing = data.isnull().sum() / len(data)
    missing = pd.DataFrame({'column_name': data.columns,'percent_missing': percent_missing})
    missing_show = missing.sort_values(by='percent_missing')
    print(missing_show[missing_show['percent_missing']>0].count())
    print('----------------------------------')
    out = missing_show[missing_show['percent_missing']>threshold]
    return out

missing(df,0.8)#可以删除缺失率大于threshold的变量

# SelectKBest可以依据相关性对特征进行选择，保留k个评分最高的特征
clean_df = df[['attr_o','sinc_o','intel_o','fun_o','amb_o','shar_o','attr','sinc','intel','fun','amb','shar','samerace','int_corr','condtn','match']].copy()
clean_df.dropna(inplace=True) #删除缺失值
label_data = clean_df['match']#target
#确定待选择的特征集
feature_data = clean_df[['attr_o','sinc_o','intel_o','fun_o','amb_o','shar_o','attr','sinc','intel','fun','amb','shar','samerace','int_corr','condtn']]

def get_feature_selectkbest(feature_data, label_data,k):
    model = SelectKBest(f_classif, k)#选择k个最佳特征:分类使用f_classif，问题使用f_regression
    X_new = model.fit_transform(feature_data, label_data)
    #feature_data是特征数据，label_data是标签数据，该函数可以选择出k个特征 
 
    print("model shape: ",X_new.shape)
 
    scores = model.scores_
    print('model scores:', scores)  # 得分越高，特征越重要
 
    p_values = model.pvalues_
    print('model p-values', p_values)  # p-values 越小，置信度越高，特征越重要
 
    # 按重要性排序，选出最重要的 k 个
    indices = np.argsort(scores)[::-1]
    k_best_features = list(feature_data.columns.values[indices[0:k]])
 
    print('k best features are: ',k_best_features)
    
    return k_best_features

get_feature_selectkbest(feature_data, label_data,k = 10)

#基于随机森林模型的特征排序
from sklearn.model_selection import cross_val_score, ShuffleSplit
from sklearn.datasets import load_boston
from sklearn.ensemble import RandomForestClassifier

y = label_data
x = feature_data
clf = RandomForestClassifier()
clf.fit(x,y)

importance = clf.feature_importances_
indices = np.argsort(importance)[::-1]
features = x.columns
for f in range(x.shape[1]):
    print(("%2d) %- *s %f" % (f+1,10,features[f],importance[indices[f]])))

#递归特征消除(RFE)，若采用的是Lasso/Ridge，正则化的回归是稳定的，那么RFE就是稳定的
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

X = feature_data
Y = label_data
 
#Feature extraction
model = LogisticRegression() 
rfe = RFE(model, 10)#底层普通回归Linear Regression,保留10个变量
fit = rfe.fit(X, Y)
 
print("Num Features: %d"% fit.n_features_) 
print("Selected Features: %s"% fit.support_)#输出是否保留数组
print("Feature Ranking: %s"% fit.ranking_)#输出特征重要性排序

#Python实现PCA
import numpy as np
def pca(X,k):#k is the components you want
  #mean of each feature
  n_samples, n_features = X.shape
  mean=np.array([np.mean(X.iloc[:,i]) for i in range(n_features)])
  #normalization
  norm_X=X-mean
  #scatter matrix
  scatter_matrix=np.dot(np.transpose(norm_X),norm_X)
  #Calculate the eigenvectors and eigenvalues
  eig_val, eig_vec = np.linalg.eig(scatter_matrix)
  eig_pairs = [(np.abs(eig_val[i]), eig_vec[:,i]) for i in range(n_features)]
  # sort eig_vec based on eig_val from highest to lowest
  eig_pairs.sort(reverse=True)
  # select the top k eig_vec
  feature=np.array([ele[1] for ele in eig_pairs[:k]])
  #get new data
  data=np.dot(norm_X,np.transpose(feature))
  return data
 
X = feature_data
outcome = pca(X,10)
print(outcome)

##用sklearn的PCA，比上面的方法更简单，结果相同
from sklearn.decomposition import PCA
import numpy as np
X = feature_data
pca=PCA(n_components=10)#保留10个主成分
pca.fit(X)
reduce_feature_data = pca.transform(X)
print(reduce_feature_data)
